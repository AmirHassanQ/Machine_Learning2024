# -*- coding: utf-8 -*-
"""MP1_Q2.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1I7cV0C4VttYtd03yA1aCCxhHmLeOVuIr

importing libraries
"""

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt

#https://drive.google.com/file/d/1DciS0Z60vxapKFGwmv2C406kBd9c_iWM/view?usp=sharing

!gdown 1DciS0Z60vxapKFGwmv2C406kBd9c_iWM

df = pd.read_csv('/content/R.csv')
df

X1_notpreproc= df[['n1']].values
X2_notpreproc= df[['f1']].values
X1_notpreproc.shape,X2_notpreproc.shape

"""#Q2_2"""

X1_preproc = X1_notpreproc[0:20000,0]

X2_preproc = X2_notpreproc[0:20000,0]

X1_notlabeled = X1_preproc.reshape(100,200)
X2_notlabeled = X2_preproc.reshape(100,200)

label1 = np.ones((100,1))
label2 = np.zeros((100,1))

X1_notlabeled.shape , X2_notlabeled.shape
X1_labeled = np.column_stack((X1_notlabeled,label1))
X2_labeled = np.column_stack((X2_notlabeled,label2))
X_labeled = np.vstack((X1_labeled,X2_labeled))

X_labeled.shape

# Mean
xf1 = np.zeros((200,1))
for i in range(200):
  xf1[i,0] = np.mean(X_labeled[i,0:198])

# Abs Mean
xf2 = np.zeros((200,1))
for i in range(200):
  xf2[i,0]=np.mean(np.abs(X_labeled[i,0:198]))

#Peak
xf3 = np.zeros((200,1))
for i in range(200):
  xf3[i,0] = np.max(X_labeled[i,0:198])

#Peak to Peak
xf4 = np.zeros((200,1))
for i in range(200):
  xf4[i,0] = np.max(X_labeled[i,0:198]) - np.min(X_labeled[i,0:198])

# smr
xf5 = np.zeros((200,1))
for i in range(200):
  xf5[i,0] = np.mean(np.sqrt(np.abs(X_labeled[i,0:198])))**2

# CLF
xf6 = np.zeros((200,1))
for i in range(200):
  xf6[i,0] = (np.max(X_labeled[i,0:198])/(np.mean(np.sqrt(np.abs(X_labeled[i,0:198])))**2))

#IF1
xf7 = np.zeros((200,1))
for i in range(200):
  xf7[i,0] = (np.max(X_labeled[i,0:198])/(np.mean(np.abs(X_labeled[i,0:198]))))

#IF2
xf8 = np.zeros((200,1))
for i in range(200):
  xf8[i,0] = np.max(X_labeled[i,0:198])/(np.mean(np.abs(X_labeled[i,0:198])))

xf_notlabeled = np.hstack((xf1,xf2,xf3,xf4,xf5,xf6,xf7,xf8))
label1 = np.ones((100,1))
label2 = np.zeros((100,1))
xf_labeled1 = np.column_stack((xf_notlabeled[0:100,0:8],label1))
xf_labeled2 = np.column_stack((xf_notlabeled[100:200,0:8],label2))

X_total = np.vstack((xf_labeled1,xf_labeled2))

from sklearn.utils import shuffle
X_shuffled = shuffle(X_total)
X = X_shuffled[0:200,0:8]
Y = X_shuffled[0:200,8]

from sklearn.model_selection import train_test_split
x_train,x_test,y_train,y_test = train_test_split(X,Y,test_size = 0.2)
x_train.shape,x_test.shape,y_train.shape,y_test.shape

from sklearn.preprocessing import MinMaxScaler
scaler = MinMaxScaler()
scaler.fit(x_train)
x_train_normalized = scaler.transform(x_train)
x_train_normalized.shape

"""# Q2_3"""

def sigmoid(x):
    return 1 / (1 + np.exp(-x))

def logistic_regression(x, w):
    y_hat = sigmoid(x @ w)
    return y_hat

def bce(y, y_hat):
    loss = -(np.mean(y*np.log(y_hat) + (1-y)*np.log(1-y_hat)))
    return loss

def gradient(x, y, y_hat):
    grads = (x.T @ (y_hat - y)) / len(y)
    return grads



def gradient_descent(w, eta, grads):
    w -= eta*grads
    return w

def accuracy(y, y_hat):
    acc = np.sum(y == np.round(y_hat)) / len(y)
    return acc

m = 7
w = np.random.randn(m+1, 1)
print(w.shape)

eta = 0.01
n_epochs = 20

(x_train_normalized.T).shape

error_hist = []

for epoch in range(n_epochs):
    # predictions
    y_hat = logistic_regression(x_train_normalized, w)
    y_hf = y_hat.flatten()

    # loss
    e = bce(y_train, y_hf)
    error_hist.append(e)


    # gradients
    grads = gradient(x_train_normalized, y_train, y_hf)


    # gradient descent
    w = gradient_descent(w, eta, grads.reshape(8,1))

    if (epoch+1) % 1 == 0:
        print(f'Epoch={epoch}, \t E={e:.4},\t w={w.T[0]}')

plt.plot(error_hist)
plt.savefig('error_hist.png')

x_test_normalized = scaler.transform(x_test)

y_hat = logistic_regression(x_test, w)

accuracy(y_test, y_hat.flatten())

y_predl = np.zeros([len(y_hat),1])
for i in range(len(y_hat)):
  if y_hat[i]>= 0.5:
    y_predl[i] = 1
  else:
    y_predl[i] = 0
y_predl

import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt
from sklearn.metrics import confusion_matrix


# Create a confusion matrix
confusion_mat = confusion_matrix(y_test, y_predl)

# Create a heatmap for the confusion matrix
plt.figure(figsize=(6, 4))
sns.heatmap(confusion_mat, annot=True, fmt='d', cmap='Blues')
plt.xlabel('Predicted Values')
plt.ylabel('True Values')
plt.title('Confusion Matrix - Correct vs. Incorrect Predictions')
plt.show()
plt.savefig('heatmap.png')

"""#Q2_4"""

from sklearn.linear_model import LogisticRegression

x1_train,x1_test,y1_train,y1_test = train_test_split(X,Y,test_size = 0.2)

model_1 = LogisticRegression(max_iter= 20)
model_1.fit(x1_train,y1_train)
model_1.predict(x1_test), y1_test

import numpy as np
import matplotlib.pyplot as plt
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score


print(y1_train.shape)
print(x1_train.shape)

# Train the model and keep track of accuracy in each iteration
accuracy_scores = []
num_iterations = 20
for i in range(num_iterations):
    # Fit the model
    model_2 = LogisticRegression(max_iter = i)
    model_2.fit(x1_train,y1_train)

    # Calculate accuracy on the training set
    y_pred = model_2.predict(x1_train)
    accuracy = accuracy_score(y1_train, y_pred)
    accuracy_scores.append(accuracy)

# Create a plot of accuracy scores in different iterations
plt.figure(figsize=(8, 5))
plt.plot(range(1, num_iterations+1), accuracy_scores, marker='o', color='b', linestyle='-')
plt.xlabel('Iterations')
plt.ylabel('Accuracy Score')
plt.title('Accuracy Score in Different Iterations')
plt.grid(True)
plt.savefig('accuracy_score')
plt.show()

