# -*- coding: utf-8 -*-
"""MP3_Q3.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1YjTrVYcIsf7xiRrApxLNGoL2Q65OmFsE

#Q3_3

Imported Libraries
"""

import numpy as np
import pandas as pd
import tensorflow as tf
import matplotlib.pyplot as plt
import seaborn as sns

# https://drive.google.com/file/d/1zm4h6k06zm5T-pNcfnOPbX0-PmRBCeCk/view?usp=sharing
!gdown 1zm4h6k06zm5T-pNcfnOPbX0-PmRBCeCk

import warnings
warnings.filterwarnings("ignore")

df = pd.read_csv('/content/creditcard.csv')
df.head()

df.describe()

# Good No Null Values!
df.isnull().sum().max()

df.columns

print('Frauds', round(df['Class'].value_counts()[1]/len(df) * 100,2), '% of the dataset')
print('NO Frauds', round(df['Class'].value_counts()[0]/len(df) * 100,2), '% of the dataset')

colors = ["#0101DF", "#DF0101"]

sns.countplot(x = 'Class', data=df, palette=colors)
plt.title('Class Distributions \n (0: No Fraud || 1: Fraud)', fontsize=14)

# Since most of our data has already been scaled we should scale the columns that are left to scale (Amount and Time)
from sklearn.preprocessing import StandardScaler, RobustScaler

# RobustScaler is less prone to outliers.

std_scaler = StandardScaler()
rob_scaler = RobustScaler()

df.head()
df.drop(['Amount'],axis = 1)
df.drop(['Time'],axis = 1)

df.drop(['Time'],axis = 1)
scaled_amoun = df['Amount']

df.insert(0,'scaled_amount',scaled_amoun)
df.head()

from sklearn.model_selection import train_test_split

X = df.drop('Class', axis=1)
y = df['Class']

x_train,x_test,y_train,y_test = train_test_split(X,y,test_size = 0.2)
print(x_train.shape)
print(x_test.shape)

print("Before OverSampling, counts of label '1': {}".format(sum(y_train == 1)))
print("Before OverSampling, counts of label '0': {} \n".format(sum(y_train == 0)))

from imblearn.over_sampling import SMOTE
sm = SMOTE(random_state = 44)
x_train_res, y_train_res = sm.fit_resample(x_train, y_train.ravel())

print('After OverSampling, the shape of train_X: {}'.format(x_train_res.shape))
print('After OverSampling, the shape of train_y: {} \n'.format(y_train_res.shape))

print("After OverSampling, counts of label '1': {}".format(sum(y_train_res == 1)))
print("After OverSampling, counts of label '0': {}".format(sum(y_train_res == 0)))

X_resampled_df = pd.DataFrame(x_train_res, columns=X.columns)
y_resampled_df = pd.DataFrame(y_train_res, columns=['Class'])

# Now you have X_resampled_df and y_resampled_df as DataFrames

# Assuming X_resampled_df and y_resampled_df are your balanced DataFrames
balanced_data = pd.concat([X_resampled_df, y_resampled_df], axis=1)

# Plot countplot
sns.countplot(data=balanced_data, x='Class')
plt.title('Balanced Dataset Distribution')
plt.xlabel('Class')
plt.ylabel('Count')
plt.show()

"""define noise"""

noise_factor = 0.2
X_train_noisy = x_train_res + noise_factor * np.random.normal(loc=0.0, scale=1.0, size=x_train_res.shape)
X_test_noisy = x_test + noise_factor * np.random.normal(loc=0.0, scale=1.0, size=x_test.shape)

import tensorflow as tf
from tensorflow.keras.models import Model
from tensorflow.keras.layers import Input, Dense
from tensorflow.keras.callbacks import ModelCheckpoint

# Build the denoising autoencoder
input_dim = x_train_res.shape[1]

input_layer = Input(shape=(input_dim,))
encoded = Dense(22, activation='relu')(input_layer)
encoded = Dense(15, activation='relu')(encoded)
encoded = Dense(10, activation='relu')(encoded)
decoded = Dense(15, activation='relu')(encoded)
decoded = Dense(22, activation='relu')(decoded)
output_layer = Dense(input_dim, activation='sigmoid')(decoded)

autoencoder = Model(input_layer, output_layer)
autoencoder.compile(optimizer='adam', loss='bce')


autoencoder_checkpoint = ModelCheckpoint('best_autoencoder.h5', monitor='val_loss', save_best_only=True, mode='min')

autoencoder.fit(X_train_noisy,x_train_res,epochs=50,batch_size=256,shuffle=True,callbacks=[autoencoder_checkpoint],verbose = 1)

# Load the best autoencoder model
#autoencoder.load_weights('best_autoencoder.h5')
# Denoise the training, validation, and test sets
X_train_denoised = autoencoder.predict(X_train_noisy)
X_test_denoised = autoencoder.predict(X_test_noisy)

"""define neural network"""

X_train_denoised.shape,y_train_res.shape

# Build the classification model
classifier_input = Input(shape=(input_dim,))
x = Dense(22, activation='relu')(classifier_input)
x = Dense(15, activation='relu')(x)
x = Dense(10, activation='relu')(x)
x = Dense(5, activation='relu')(x)
x = Dense(2, activation='softmax')(x)

classifier = Model(classifier_input, x)
classifier.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])

# Define checkpoint callback to save the best classifier model
classifier_checkpoint = ModelCheckpoint('best_classifier.h5', monitor='val_loss', save_best_only=True, mode='min')

# Train the classifier
classifier.fit(X_train_denoised,y_train_res,epochs=50,batch_size=256,shuffle = True,callbacks=[classifier_checkpoint],verbose = 1)

y_test.shape

#classifier.load_weights('best_classifier.h5')
from sklearn.metrics import accuracy_score, confusion_matrix, classification_report,f1_score,recall_score,precision_score
# Predict on the denoised test set
Y_test_pred = classifier.predict(X_test_denoised)
Y_test_pred_classes = np.argmax(Y_test_pred, axis=1)

Y_test_true = y_test


# Calculate metrics
conf_matrix = confusion_matrix(Y_test_true, Y_test_pred_classes)
accuracy = accuracy_score(Y_test_true, Y_test_pred_classes)
f1 = f1_score(Y_test_true, Y_test_pred_classes)
recall = recall_score(Y_test_true, Y_test_pred_classes)
precision = precision_score(Y_test_true, Y_test_pred_classes)

# Print metrics
print(f'Accuracy: {accuracy*100:.2f}%')
print(f'F1 Score: {f1:.2f}')
print(f'Recall: {recall:.2f}')
print(f'Precision: {precision:.2f}')

print(classification_report(Y_test_true, Y_test_pred_classes))
# Plot confusion matrix
plt.figure(figsize=(8, 6))
sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues')
plt.title('Confusion Matrix')
plt.xlabel('Predicted')
plt.ylabel('Actual')
plt.show()

thresholds = np.arange(0.0, 1.05, 0.05)

recalls = []
accuracies = []
for threshold in thresholds:
    y_pred = (Y_test_pred[:, 1] >= threshold).astype(int)
    recall = recall_score(y_test, y_pred)
    accuracy = accuracy_score(y_test, y_pred)
    recalls.append(recall)
    accuracies.append(accuracy)

plt.figure(figsize=(10, 6))
plt.plot(thresholds, recalls, label='Recall')
plt.plot(thresholds, accuracies, label='Accuracy')
plt.xlabel('Threshold (0~1)')
plt.ylabel('Recall & Accuracy')
plt.title('Recall & Accuracy vs. Threshold')
plt.legend()
plt.show()