# -*- coding: utf-8 -*-
"""MP3_Q1.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1jNqtWDnNGRxj-MDvURIgIp7dIBG_N_-w

#Q1_1

importing libraries
"""

from sklearn.datasets import load_iris
import numpy as np
import matplotlib.pyplot as plt
import pandas as pd
import seaborn as sns

iris = load_iris()
iris.feature_names
df = pd.DataFrame(iris.data,columns = iris.feature_names)
df['target'] = iris.target
df

df['sepal length (cm)'].plot(kind='hist', bins=20, title='sepal length (cm)')
plt.gca().spines[['top', 'right',]].set_visible(False)

df['petal length (cm)'].plot(kind='hist', bins=20, title='petal length (cm)')
plt.gca().spines[['top', 'right',]].set_visible(False)

df['sepal width (cm)'].plot(kind='hist', bins=20, title='sepal width (cm)')
plt.gca().spines[['top', 'right',]].set_visible(False)

df['petal width (cm)'].plot(kind='hist', bins=20, title='petal width (cm)')
plt.gca().spines[['top', 'right',]].set_visible(False)

iris_dna = df.dropna(axis=0, how='any')
iris_dna.head()

iris_dna.info()

iris_dna.describe()

# Scatter plot of the dataset
sns.pairplot(iris_dna,hue="target")
plt.show()

iris = load_iris()
X = iris.data
y = iris.target

# dimansion
print('dimansion of data:',X.shape)
print('dimansion of targets:',y.shape)
# number of samples
print('number of samples:',len(X))

# Mean
xf1 = np.zeros((4,1))
for i in range(4):
  xf1[i,0] = np.mean(X[0:150,i])
  print('mean of feature '+str(i)+': '+str(xf1[i,0]))

# Variance
xf2 = np.zeros((4,1))
for i in range(4):
  xf2[i,0] = np.var(X[0:150,i])
  print('variance of feature '+str(i)+': '+str(xf2[i,0]))

iris_dna.corr()

def plot_iris_2d(x, y, title, xlabel="1st eigenvector", ylabel="2nd eigenvector"):
    sns.set_style("darkgrid")

    plt.scatter(x, y,c=iris.target)

    plt.title(title, fontsize=20)
    plt.legend(["0","1","2"],loc="lower left")

    plt.xlabel(xlabel, fontsize=16)
    plt.ylabel(ylabel, fontsize=16)

from sklearn.manifold import TSNE

tsne = TSNE(n_components=2, n_iter=1000, random_state= 44)
points = tsne.fit_transform(X)

plot_iris_2d(x = points[:, 0],y = points[:, 1],title = 'Iris dataset visualized with t-SNE')

"""#Q1_2"""

from sklearn.decomposition import PCA
from sklearn.discriminant_analysis import LinearDiscriminantAnalysis
from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis
from sklearn.preprocessing import MinMaxScaler

scaler = MinMaxScaler()
scaler.fit(X)
x_normalized = scaler.transform(X)
x_normalized.shape

lda1 = LinearDiscriminantAnalysis(n_components=2)
qda1 = QuadraticDiscriminantAnalysis()
pca1 = PCA(n_components=2)

"""LDA implementation"""

X_train_lda = lda1.fit_transform(X, y)

data1=pd.DataFrame(X_train_lda)
data1['class']=y
data1.columns=["LD1","LD2","class"]
#data1.head()

X_train_lda.shape

markers = ['s', 'x','o']
colors = ['r', 'b','g']
sns.lmplot(x="LD1", y="LD2", data=data1, hue='class', markers=markers,fit_reg=False,legend=False)
plt.legend(loc='upper left')
plt.show()

data1.corr()

"""pca implimentation"""

X_train_pca = pca1.fit_transform(X, y)

data2=pd.DataFrame(X_train_pca)
data2['class']=y
data2.columns=["PC1","PC2","class"]
#data1.head()

X_train_pca.shape

markers = ['s', 'x','o']
colors = ['r', 'b','g']
sns.lmplot(x="PC1", y="PC2", data=data2, hue='class', markers=markers,fit_reg=False,legend=False)
plt.legend(loc='upper center')
plt.show()

data1.corr()

data2.corr()

from sklearn.model_selection import train_test_split

x_train,x_test,y_train,y_test = train_test_split(X_train_pca,y,test_size = 0.2)
x_train.shape

from sklearn.svm import SVC
import matplotlib.pyplot as plt
import numpy as np


clf = SVC(kernel='linear')
clf.fit(x_train,y_train)

"""SVM"""

import matplotlib.pyplot as plt

from sklearn import datasets, svm
from sklearn.inspection import DecisionBoundaryDisplay

# we create an instance of SVM and fit out data. We do not scale our
# data since we want to plot the support vectors
C = 1.0  # SVM regularization parameter
models = (
    svm.SVC(kernel="linear", C=C),
    svm.LinearSVC(C=C, max_iter=10000),
    svm.SVC(kernel="rbf", gamma=0.7, C=C),
    svm.SVC(kernel="poly", degree=3, gamma="auto", C=C),
)
models = (clf.fit(x_train, y_train) for clf in models)

# title for the plots
titles = (
    "SVC with linear kernel",
    "LinearSVC (linear kernel)",
    "SVC with RBF kernel",
    "SVC with polynomial (degree 3) kernel",
)

# Set-up 2x2 grid for plotting.
fig, sub = plt.subplots(2, 2)
plt.subplots_adjust(wspace=0.4, hspace=0.4)

X0, X1 = x_train[:, 0], x_train[:, 1]

for clf, title, ax in zip(models, titles, sub.flatten()):
    disp = DecisionBoundaryDisplay.from_estimator(
        clf,
        x_train,
        response_method="predict",
        cmap=plt.cm.coolwarm,
        alpha=0.8,
        ax=ax,
        xlabel=iris.feature_names[0],
        ylabel=iris.feature_names[1],
    )
    ax.scatter(X0, X1, c=y_train, cmap=plt.cm.coolwarm, s=20, edgecolors="k")
    ax.set_xticks(())
    ax.set_yticks(())
    ax.set_title(title)
    print(title+'score'+str(clf.score(x_train,y_train)))

plt.show()

"""#Q1_3

"""

import matplotlib.pyplot as plt

from sklearn import datasets, svm
from sklearn.inspection import DecisionBoundaryDisplay

C = 1.0  # SVM regularization parameter
model = svm.SVC(kernel="poly", gamma="auto", C=C)

# Set-up 5x2 grid for plotting.
fig, sub = plt.subplots(5,2)
plt.subplots_adjust(wspace=0.4, hspace=0.4)

X0, X1 = x_train[:, 0], x_train[:, 1]

for i, ax in zip(range(10),sub.flatten()):
  clf = model.fit(x_train,y_train)
  clf.degree = i+1
  disp = DecisionBoundaryDisplay.from_estimator(clf,x_train , response_method="predict",cmap=plt.cm.coolwarm,alpha=0.8,ax=ax)
  ax.scatter(X0, X1, c=y_train, cmap=plt.cm.coolwarm, s=20, edgecolors="k")
  print('score'+str(i+1)+'='+str(clf.score(x_train,y_train)))
  ax.set_title('polynomial kernel with degree'+str(i+1))
  plt.savefig('SVC polynomial kernel with degree'+str(i+1))


plt.show()

import imageio
image1 = imageio.imread("/content/SVC polynomial kernel with degree1.png")
image2 = imageio.imread("/content/SVC polynomial kernel with degree2.png")
image3 = imageio.imread("/content/SVC polynomial kernel with degree3.png")
image4 = imageio.imread("/content/SVC polynomial kernel with degree4.png")
image5 = imageio.imread("/content/SVC polynomial kernel with degree5.png")
image6 = imageio.imread("/content/SVC polynomial kernel with degree6.png")
image7 = imageio.imread("/content/SVC polynomial kernel with degree7.png")
image8 = imageio.imread("/content/SVC polynomial kernel with degree8.png")
image9 = imageio.imread("/content/SVC polynomial kernel with degree9.png")
image10 = imageio.imread("/content/SVC polynomial kernel with degree10.png")

import imageio.v3 as iio


filenames = ['/content/SVC polynomial kernel with degree1.png', '/content/SVC polynomial kernel with degree2.png','/content/SVC polynomial kernel with degree3.png','/content/SVC polynomial kernel with degree4.png','/content/SVC polynomial kernel with degree5.png','/content/SVC polynomial kernel with degree6.png','/content/SVC polynomial kernel with degree7.png','/content/SVC polynomial kernel with degree8.png','/content/SVC polynomial kernel with degree9.png','/content/SVC polynomial kernel with degree10.png']
image = [ ]

for filename in filenames:
  image.append(iio.imread(filename))

iio.imwrite('result.gif', image, duration = 500, loop = 0)

"""#Q1_4"""

import numpy as np
import matplotlib.pyplot as plt

class SVM:
    def __init__(self, degree=3, learning_rate=0.01, num_epochs=1000):
        self.degree = degree
        self.learning_rate = learning_rate
        self.num_epochs = num_epochs
        self.weights = 0
        self.bias = 0

    def polynomial_kernel(self,X):
        return (1 + np.dot(X[:,0],X[:,1])) ** self.degree

    def fit(self, X, y):
        num_samples, num_features = X.shape
        self.weights = np.zeros(num_features)
        self.bias = 0

        for _ in range(self.num_epochs):
            for i in range(num_samples):
                condition = y[i] * (np.dot(X[i], self.weights) + self.bias) >= 1
                if condition:
                    self.weights -= self.learning_rate * (2 * 1 / self.num_epochs * self.weights)
                else:
                    self.weights -= self.learning_rate * (2 * 1 / self.num_epochs * self.weights - np.dot(X[i], y[i]))
                    self.bias -= self.learning_rate * y[i]

    def predict(self, X):
        linear_output = np.dot(X, self.weights) + self.bias
        return np.sign(linear_output)

    def accuracy(self,X,y):
      y_hat = np.sign(np.dot(X, self.weights) + self.bias)
      acc = np.sum(y == y_hat) / len(y)
      return acc

from sklearn.inspection import DecisionBoundaryDisplay

model = SVM(degree = 3)
clf = model.fit(x_train,y_train)
#disp = DecisionBoundaryDisplay.from_estimator(clf,x_train ,cmap=plt.cm.coolwarm,alpha=0.8,ax=ax)
# Set-up 5x2 grid for plotting.
'''
fig, sub = plt.subplots(5,2)
plt.subplots_adjust(wspace=0.4, hspace=0.4)

X0, X1 = x_train[:, 0], x_train[:, 1]

for i, ax in zip(range(10),sub.flatten()):
  model.degree = i+1
  clf = model.fit(x_train,y_train)
  disp = DecisionBoundaryDisplay.from_estimator(clf,x_train ,cmap=plt.cm.coolwarm,alpha=0.8,ax=ax)
  ax.scatter(X0, X1, c=y_train, cmap=plt.cm.coolwarm, s=20, edgecolors="k")
  print('score'+str(i+1)+'='+str(clf.accuracy(x_train,y_train)))
  ax.set_title('polynomial kernel with degree'+str(i+1))
  plt.savefig('degree'+str(i+1))


plt.show()'''

data2.columns = ['x1','x2','y']

import numpy as np


class SVM:

    def __init__(self, C = 1.0,degree = 3,kernel = None):
        # C = error term
        self.C = C
        self.w = 0
        self.b = 0
        self.kernel = kernel
        self.degree = degree

    # Hinge Loss Function / Calculation
    def hingeloss(self, w, b, x, y):
        # Regularizer term
        reg = 0.5 * (w * w)

        for i in range(x.shape[0]):
            # Optimization term
            opt_term = y[i] * ((np.dot(w, x[i])) + b)

            # calculating loss
            loss = reg + self.C * max(0, 1-opt_term)
        return loss[0][0]

    def transform_poly(self, X, Y=None):
             df = pd.DataFrame()
        # Finding the Square of X1, X2
            #X['x1^2'] = X['x1'] ** self.degree
            #X['x2^2'] = X['x2'] ** self.degree
            # Finding the product of X1 and X2
            df['x1 * x2'] = (1+X['x1'] * X['x2'])**self.degree
            # Converting dataset to numpy array
            df = df.to_numpy()
            if Y.size != 0:
                Y = Y.to_numpy()
                return df, Y
            else:
                return df

    def fit(self, X, Y, batch_size=100, learning_rate=0.001, epochs=1000):
        if(self.kernel == "poly"):
            print("SVM(kernel='poly')")
            X, Y = self.transform_poly(X, Y)
        else:
            X = X.to_numpy()
            Y = Y.to_numpy()

        # The number of features in X
        number_of_features = X.shape[1]

        # The number of Samples in X
        number_of_samples = X.shape[0]

        c = self.C

        # Creating ids from 0 to number_of_samples - 1
        ids = np.arange(number_of_samples)

        # Shuffling the samples randomly
        np.random.shuffle(ids)

        # creating an array of zeros
        w = np.zeros((1, number_of_features))
        b = 0
        losses = []

        # Gradient Descent logic
        for i in range(epochs):
            # Calculating the Hinge Loss
            l = self.hingeloss(w, b, X, Y)

            # Appending all losses
            losses.append(l)

            # Starting from 0 to the number of samples with batch_size as interval
            for batch_initial in range(0, number_of_samples, batch_size):
                gradw = 0
                gradb = 0

                for j in range(batch_initial, batch_initial+ batch_size):
                    if j < number_of_samples:
                        x = ids[j]
                        ti = Y[x] * (np.dot(w, X[x].T) + b)

                        if ti > 1:
                            gradw += 0
                            gradb += 0
                        else:
                            # Calculating the gradients

                            #w.r.t w
                            gradw += c * Y[x] * X[x]
                            # w.r.t b
                            gradb += c * Y[x]

                # Updating weights and bias
                w = w - learning_rate * w + learning_rate * gradw
                b = b + learning_rate * gradb

        self.w = w
        self.b = b

        return self.w, self.b, losses

    def predict(self, X):

        prediction = np.dot(X, self.w[0]) + self.b # w.x + b
        return np.sign(prediction)

    def accuracy(self,X,y):
      y_hat = np.sign(np.dot(X, self.w[0]) + self.b)
      acc = np.sum(y == y_hat) / len(y)
      return acc

from sklearn.inspection import DecisionBoundaryDisplay
x_train = data2[['x1','x2']]
y_train = data2['y']
model = SVM(degree = 2,kernel='poly')
clf = model.fit(x_train,y_train)
model.accuracy(x_train,y_train)
data2